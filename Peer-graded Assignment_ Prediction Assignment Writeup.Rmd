---
title: 'Peer-graded Assignment: Prediction Assignment Writeup'
author: "Harshitha"
date: "1/31/2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

Practical Machine Learning Project:

#Introduction-
The goal of this project is to predict the quality of exercise performance using data collected from wearable devices like Jawbone Up and Fitbit. The data comes from a study where participants performed Unilateral Dumbbell Biceps Curls in five different ways: one correct (Class A) and four incorrect (Classes B, C, D, and E). The task is to build a machine learning model that can classify the exercise performance based on sensor data.
The analysis follows a structured approach: data retrieval, cleaning, feature selection, model building, and evaluation. Below is a detailed walkthrough of the process.

###Data Retrieval and Cleaning

Loading Libraries and Data
First, the necessary libraries are loaded, and the training and testing datasets are retrieved directly from the provided URLs.

```{r}
library(data.table)
library(mlbench)
library(caret)
library(klaR)
library(randomForest)
library(rattle)
library(rpart)
library(rpart.plot)
```
I don't need to download the data in my pc, I can load them to memory.
```{r}

TrainingData <- fread("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", header=T, na.strings=c("NA","#DIV/0!",""))
TestingData <- fread("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", header=T, na.strings=c("NA","#DIV/0!",""))
dim(TrainingData) # 19622   160
dim(TestingData) # 20 160
```

###Exploring the Data
The dataset contains 160 columns, many of which have missing values or are irrelevant for this analysis.
To focus on meaningful predictors, columns with missing values and those unrelated to sensor measurements (e.g., user names, timestamps) are removed.

```{r}
names(TrainingData)
```

# Identify columns with missing values

```{r}

List_Na <- sapply(TrainingData, function(x) any(is.na(x)))
newTrainingData <- subset(TrainingData, 
                          select = c("classe", names(List_Na)[!List_Na & grepl("belt|[^(fore)]arm|dumbbell|forearm", names(List_Na))]))

newTrainingData <- newTrainingData[, classe := factor(newTrainingData[, classe])]

```


###Splitting the Data
#The training data is split into two subsets: 60% for training and 40% for validation.

```{r}
inTrain <- createDataPartition(newTrainingData$classe, p = 0.6, list = FALSE)
Train_data <- newTrainingData[inTrain, ]
Test_data <- newTrainingData[-inTrain, ]
```

#Then I check if in my batch there are near-zero variance predictors
#Near-zero variance predictors are identified and removed to avoid overfitting.
```{r}
nzv <- nearZeroVar(Train_data, saveMetrics=TRUE)
nzv
```


###Model Building

#Regularized Discriminant Analysis (RDA)
#An RDA model is trained.
# Train RDA model
```{r}
set.seed(123)
rdaFit <- train(classe ~ ., data = Train_data, method = "rda", tuneGrid = data.frame(gamma = (0:4)/4, lambda = 3/4), trControl = ctrl, metric = "ROC")
rdaFit
rdaClasses <- predict(rdaFit, newdata = Test_data)
confusionMatrix(rdaClasses, Test_data$classe)
```
The final values used for the model were gamma = 0 and lambda = 0.75.


#Partial Least Squares Discriminant Analysis (PLSDA)
#A PLSDA model is trained using 10-fold cross-validation with 3 repeats. 
#The model is evaluated using the ROC metric.
Train PLSDA model
```{r}
plsClasses <- predict(plsFit, newdata = Test_data)
str(plsClasses)
```
Predict on test data
```{r}
plsProbs <- predict(plsFit, newdata = Test_data, type = "prob")
head(plsProbs)
```
Plot model performance
```{r}
trellis.par.set(caretTheme())
plot(plsFit, metric = "Kappa")
```
```{r}
confusionMatrix(plsClasses, Test_data$classe)
```

#Random Forest
#A Random Forest model is trained, achieving high accuracy.
# Train Random Forest model

```{r}
rfFit <- train(classe~., data=Train_data, method="rf", tuneGrid=expand.grid(.mtry=sqrt(ncol(Train_data[,2:53]))), trControl=ctrl)
rfFit
rfClasses <- predict(rfFit, newdata = Test_data)
confusionMatrix(rfClasses, Test_data$classe)
```

The Random Forest model achieves an accuracy of 99.5%, making it the best-performing model.

###first two model Evaluation
The performance of the PLSDA and RDA models is compared using resampling results.
Compare models

```{r}
resamps <- resamples(list(pls = plsFit, rda = rdaFit))
summary(resamps)
diffs <- diff(resamps)
summary(diffs)
```
Visualize comparison
```{r}
xyplot(resamps, what = "BlandAltman")
```

#Variable Importance
#The importance of variables in the Random Forest model is examined.
# Extract variable importance
```{r}
varImp(rfFit)
rfFit$finalModel
```

###Conclusion
The Random Forest model is selected as the final model due to its high accuracy (99.5%). It is then used to predict the exercise quality for the test dataset.
Predict on the original test data

```{r}
TestResult <- predict(rfFit, newdata=TestingData)
TestResult
```









